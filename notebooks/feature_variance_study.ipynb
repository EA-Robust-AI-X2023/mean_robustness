{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1385fbc7",
   "metadata": {},
   "source": [
    "## Study of the class-variance of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99aafb8",
   "metadata": {},
   "source": [
    "This notebook is dedicated to studying the class-variance of features of the datasets used to test wether the mean is more robust than robust aggregators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5c9699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from byzfl import DataDistributor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db9e6e",
   "metadata": {},
   "source": [
    "### Extraits de byzfl (datasets disponibles, transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e872bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 0 1]\n",
      "[[0.82 0.   0.2  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.18 0.   0.8  0.  ]\n",
      " [0.   0.   0.   1.  ]]\n",
      "[[41  0 10  0]\n",
      " [ 0 50  0  0]\n",
      " [ 9  0 40  0]\n",
      " [ 0  0  0 50]]\n"
     ]
    }
   ],
   "source": [
    "partition = np.array([[10,0,41,0], [0,0,0,50], [40,0,9,0], [0,50,0,0]])\n",
    "majority_classes = np.argmax(partition, axis = 1) #for each client, we compute the class which is in majority\n",
    "majority_shares= [partition[majority_class[i]] for i, partition  in enumerate(partitions)]]/partition.sum(axis=0)\n",
    "print(majority_classes)\n",
    "print(majority_shares)\n",
    "print(partition[:, majority_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514a6d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets available in byzfl. For now, we do not apply the normalizations because in \"mean is more robust\", there does not seem to be any.\n",
    "\n",
    "# transforms_hflip = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
    "# transforms_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# transforms_cifar_train = transforms.Compose([\n",
    "#         transforms.RandomCrop(32, padding=4),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "# ])\n",
    "# transforms_cifar_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "# ])\n",
    "\n",
    "# #Supported datasets\n",
    "# dict_datasets = {\n",
    "#     \"mnist\":        (\"MNIST\", transforms_mnist, transforms_mnist),\n",
    "#     \"fashionmnist\": (\"FashionMNIST\", transforms_hflip, transforms_hflip),\n",
    "#     \"emnist\":       (\"EMNIST\", transforms_mnist, transforms_mnist),\n",
    "#     \"cifar10\":      (\"CIFAR10\", transforms_cifar_train, transforms_cifar_test),\n",
    "#     \"cifar100\":     (\"CIFAR100\", transforms_cifar_train, transforms_cifar_test),\n",
    "#     \"imagenet\":     (\"ImageNet\", transforms_hflip, transforms_hflip)\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b8548a",
   "metadata": {},
   "source": [
    "### Plot des distributions et variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"MNIST\"\n",
    "\n",
    "dataset = getattr(datasets, dataset_name)(\n",
    "            root = \"../data\", \n",
    "            train = True, \n",
    "            download = True,\n",
    "            transform = None\n",
    "    )\n",
    "\n",
    "if dataset_name in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "        dataset.dataset.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "### On reprend ici le code de DataDistributor de byzfl, mais en modifiant le split pour ne pas obtenir de dataloader (plus simple pour plotter)\n",
    "def create_data_splits(data_loader, data_distribution_name, distribution_parameter, nb_honest, batch_size, min_size):\n",
    "    params= {\n",
    "        \"data_loader\": data_loader,\n",
    "        \"data_distribution_name\": data_distribution_name,\n",
    "        \"nb_workers\": nb_honest,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"distribution_parameter\": distribution_parameter\n",
    "    }\n",
    "    distributor = DataDistributor(params)\n",
    "\n",
    "    ## Ensuite, on reprend la méthode \"split data\" de DataDistributor, sans crée de dataloader afin de faciliter le plotting:\n",
    "\n",
    "    targets = data_loader.dataset.targets\n",
    "    if isinstance(data_loader, DataLoader):\n",
    "        idx = list(range(len(targets)))\n",
    "    else:\n",
    "        idx = data_loader.indices\n",
    "\n",
    "    if data_distribution_name == \"iid\":\n",
    "        split_idx = distributor.iid_idx(idx)\n",
    "    elif data_distribution_name == \"gamma_similarity_niid\":\n",
    "        split_idx = distributor.gamma_niid_idx(targets, idx)\n",
    "    elif data_distribution_name == \"dirichlet_niid\":\n",
    "        split_idx = distributor.dirichlet_niid_idx(targets, idx)\n",
    "    elif data_distribution_name == \"extreme_niid\":\n",
    "        split_idx = distributor.extreme_niid_idx(targets, idx)\n",
    "    elif data_distribution_name == \"dirichlet_niid_modified\":\n",
    "        split_idx = distributor.dirichlet_niid_modified_idx(targets, idx, min_size=min_size)\n",
    "    elif data_distribution_name == \"extreme_niid_modified\":\n",
    "        split_idx = distributor.extreme_niid_modified_idx(targets, idx)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid value for data_distribution_name: {data_distribution_name}\")\n",
    "        \n",
    "    ##ENfin, on récupère les données:\n",
    "\n",
    "    subsets=[]\n",
    "    for i in range(len(split_idx)):\n",
    "        subset = Subset(data_loader.dataset, split_idx[i])\n",
    "        subsets.append(subset)\n",
    "    return subsets\n",
    "\n",
    "\n",
    "#Définir les paramètres de la distribution\n",
    "batch_size = 128 #anecdotique car on ne crée pas de dataloader\n",
    "\n",
    "data_distributions=[\"dirichlet_niid\", \"dirichlet_niid_modified\"]\n",
    "\n",
    "dirichlet_parameter_list=[(0.01, 3000),(0.03, 3000),(0.1,3000), (1.0, 10)]\n",
    "\n",
    "\n",
    "nb_honest = 10 #nombre de workers\n",
    "seed=1\n",
    "\n",
    "# random.seed(seed)\n",
    "# # NumPy\n",
    "# np.random.seed(seed)\n",
    "# os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "data_splits = {dist+\" α=\"+str(dirichlet_param) if dirichlet_param else dist: create_data_splits(DataLoader(dataset, batch_size=batch_size, shuffle=True), dist, dirichlet_param[0], nb_honest, batch_size, min_size=dirichlet_param[1]) for dist in data_distributions for dirichlet_param in (dirichlet_parameter_list if \"dirichlet_niid\" in dist else (0.0,10))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9396263a",
   "metadata": {},
   "source": [
    "### On peut maintenant plot les distributions par classe par worker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff509db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def plot_worker_distributions_compact(data_splits, num_classes=10):\n",
    "    \"\"\"\n",
    "    data_splits: dict\n",
    "        Clés = noms des distributions (ex: 'Dirichlet α=0.1')\n",
    "        Valeurs = liste de Subset (un Subset par worker)\n",
    "    \"\"\"\n",
    "    dist_names = list(data_splits.keys())\n",
    "    n_dists = len(dist_names)\n",
    "    colors = plt.cm.tab10.colors  \n",
    "\n",
    "    fig, axes = plt.subplots(n_dists // 2, 2, figsize=(10,20), sharex=True, constrained_layout=True)\n",
    "    if n_dists == 1:\n",
    "        axes = [axes]  # pour itérer même s’il y a un seul plot\n",
    "\n",
    "    for ax, dist_name in zip(np.array(axes).reshape(n_dists // 2,2).flatten(order='F'), dist_names):\n",
    "        subsets = data_splits[dist_name]\n",
    "        num_workers = len(subsets)\n",
    "\n",
    "        # Calcul des proportions par worker\n",
    "        proportions = []\n",
    "        for subset in subsets:\n",
    "            targets = np.array([subset.dataset.targets[idx] for idx in subset.indices])\n",
    "            counts = Counter(targets)\n",
    "            total = len(targets)\n",
    "            prop = [counts.get(c, 0) for c in range(num_classes)]\n",
    "            proportions.append(prop)\n",
    "        proportions = np.array(proportions)\n",
    "\n",
    "        # Plot horizontal empilé\n",
    "        left = np.zeros(num_workers)\n",
    "        for c in range(num_classes):\n",
    "            ax.barh(range(num_workers), proportions[:, c], left=left,\n",
    "                    color=colors[c], label=f'Classe {c}' if ax == axes[0][0] else None)\n",
    "            left += proportions[:, c]\n",
    "\n",
    "        ax.set_yticks(range(num_workers))\n",
    "        ax.set_yticklabels([f'{i+1}' for i in range(num_workers)])\n",
    "        ax.set_title(dist_name, fontsize=14, weight='bold')\n",
    "        ax.invert_yaxis()  # pour que W0 soit en haut\n",
    "\n",
    "    for i in range((n_dists) // 2):\n",
    "        axes[i][0].set_ylabel('Workers')\n",
    "    axes[-1][0].set_xlabel('Number by class')\n",
    "    axes[-1][1].set_xlabel('Number by class')\n",
    "    axes[0][0].legend(title='Classes', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.suptitle(\"Distribution by class per worker (min size = 3000)\", fontsize=16, weight='bold')\n",
    "    plt.savefig(\"worker_distributions_compact_number.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "plot_worker_distributions_compact(data_splits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d79e0",
   "metadata": {},
   "source": [
    "### Enfin, on calcule la variance des features par worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "def compute_feature_variances_flatten(subsets):\n",
    "    \"\"\"\n",
    "    Pour chaque worker, flatten toutes les images en vecteur et calculer\n",
    "    la variance globale sur tous les pixels.\n",
    "    \"\"\"\n",
    "    variances = []\n",
    "    for subset in subsets:\n",
    "        # flatten toutes les images et convertir en Tensor si nécessaire\n",
    "        # all_pixels = torch.cat([\n",
    "        #     to_tensor(subset.dataset[i][0]).view(-1).float()\n",
    "        #     if not isinstance(subset.dataset[i][0], torch.Tensor)\n",
    "        #     else subset.dataset[i][0].view(-1).float()\n",
    "        #     for i in subset.indices\n",
    "        # ])\n",
    "        all_pixels = subset.dataset.data[subset.indices].view(-1).float()\n",
    "        # variance globale sur tous les pixels\n",
    "        variances.append(torch.var(all_pixels).item())\n",
    "    return np.array(variances)\n",
    "\n",
    "\n",
    "\n",
    "def plot_feature_variances_compact(data_splits):\n",
    "    \"\"\"\n",
    "    data_splits: dict\n",
    "        Clés = noms des distributions\n",
    "        Valeurs = liste de Subset (un par worker)\n",
    "    \"\"\"\n",
    "    dist_names = list(data_splits.keys())\n",
    "    n_dists = len(dist_names)\n",
    "\n",
    "    fig, axes = plt.subplots(n_dists//2, 2, figsize=(10,10), sharey=True)\n",
    "    if n_dists == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, dist_name in zip(axes.flat, dist_names):\n",
    "        subsets = data_splits[dist_name]\n",
    "        variances = compute_feature_variances_flatten(subsets)\n",
    "        num_workers = len(variances)\n",
    "\n",
    "        ax.bar(range(num_workers), variances, color=\"skyblue\", edgecolor=\"black\")\n",
    "        ax.set_title(dist_name, fontsize=14, weight='bold')\n",
    "        ax.set_xlabel(\"Worker\")\n",
    "        ax.set_ylabel(\"Variance moyenne des features\")\n",
    "\n",
    "        ax.set_xticks(range(num_workers))\n",
    "        ax.set_xticklabels([f\"W{i}\" for i in range(num_workers)])\n",
    "\n",
    "    plt.suptitle(\"Variance moyenne des features par worker selon la distribution\", fontsize=16, weight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    \n",
    "plot_feature_variances_compact(data_splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc1299",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
